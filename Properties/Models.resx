<?xml version="1.0" encoding="utf-8"?>
<root>
  <!-- 
    Microsoft ResX Schema 
    
    Version 2.0
    
    The primary goals of this format is to allow a simple XML format 
    that is mostly human readable. The generation and parsing of the 
    various data types are done through the TypeConverter classes 
    associated with the data types.
    
    Example:
    
    ... ado.net/XML headers & schema ...
    <resheader name="resmimetype">text/microsoft-resx</resheader>
    <resheader name="version">2.0</resheader>
    <resheader name="reader">System.Resources.ResXResourceReader, System.Windows.Forms, ...</resheader>
    <resheader name="writer">System.Resources.ResXResourceWriter, System.Windows.Forms, ...</resheader>
    <data name="Name1"><value>this is my long string</value><comment>this is a comment</comment></data>
    <data name="Color1" type="System.Drawing.Color, System.Drawing">Blue</data>
    <data name="Bitmap1" mimetype="application/x-microsoft.net.object.binary.base64">
        <value>[base64 mime encoded serialized .NET Framework object]</value>
    </data>
    <data name="Icon1" type="System.Drawing.Icon, System.Drawing" mimetype="application/x-microsoft.net.object.bytearray.base64">
        <value>[base64 mime encoded string representing a byte array form of the .NET Framework object]</value>
        <comment>This is a comment</comment>
    </data>
                
    There are any number of "resheader" rows that contain simple 
    name/value pairs.
    
    Each data row contains a name, and value. The row also contains a 
    type or mimetype. Type corresponds to a .NET class that support 
    text/value conversion through the TypeConverter architecture. 
    Classes that don't support this are serialized and stored with the 
    mimetype set.
    
    The mimetype is used for serialized objects, and tells the 
    ResXResourceReader how to depersist the object. This is currently not 
    extensible. For a given mimetype the value must be set accordingly:
    
    Note - application/x-microsoft.net.object.binary.base64 is the format 
    that the ResXResourceWriter will generate, however the reader can 
    read any of the formats listed below.
    
    mimetype: application/x-microsoft.net.object.binary.base64
    value   : The object must be serialized with 
            : System.Runtime.Serialization.Formatters.Binary.BinaryFormatter
            : and then encoded with base64 encoding.
    
    mimetype: application/x-microsoft.net.object.soap.base64
    value   : The object must be serialized with 
            : System.Runtime.Serialization.Formatters.Soap.SoapFormatter
            : and then encoded with base64 encoding.

    mimetype: application/x-microsoft.net.object.bytearray.base64
    value   : The object must be serialized into a byte array 
            : using a System.ComponentModel.TypeConverter
            : and then encoded with base64 encoding.
    -->
  <xsd:schema id="root" xmlns="" xmlns:xsd="http://www.w3.org/2001/XMLSchema" xmlns:msdata="urn:schemas-microsoft-com:xml-msdata">
    <xsd:import namespace="http://www.w3.org/XML/1998/namespace" />
    <xsd:element name="root" msdata:IsDataSet="true">
      <xsd:complexType>
        <xsd:choice maxOccurs="unbounded">
          <xsd:element name="metadata">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name="value" type="xsd:string" minOccurs="0" />
              </xsd:sequence>
              <xsd:attribute name="name" use="required" type="xsd:string" />
              <xsd:attribute name="type" type="xsd:string" />
              <xsd:attribute name="mimetype" type="xsd:string" />
              <xsd:attribute ref="xml:space" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name="assembly">
            <xsd:complexType>
              <xsd:attribute name="alias" type="xsd:string" />
              <xsd:attribute name="name" type="xsd:string" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name="data">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name="value" type="xsd:string" minOccurs="0" msdata:Ordinal="1" />
                <xsd:element name="comment" type="xsd:string" minOccurs="0" msdata:Ordinal="2" />
              </xsd:sequence>
              <xsd:attribute name="name" type="xsd:string" use="required" msdata:Ordinal="1" />
              <xsd:attribute name="type" type="xsd:string" msdata:Ordinal="3" />
              <xsd:attribute name="mimetype" type="xsd:string" msdata:Ordinal="4" />
              <xsd:attribute ref="xml:space" />
            </xsd:complexType>
          </xsd:element>
          <xsd:element name="resheader">
            <xsd:complexType>
              <xsd:sequence>
                <xsd:element name="value" type="xsd:string" minOccurs="0" msdata:Ordinal="1" />
              </xsd:sequence>
              <xsd:attribute name="name" type="xsd:string" use="required" />
            </xsd:complexType>
          </xsd:element>
        </xsd:choice>
      </xsd:complexType>
    </xsd:element>
  </xsd:schema>
  <resheader name="resmimetype">
    <value>text/microsoft-resx</value>
  </resheader>
  <resheader name="version">
    <value>2.0</value>
  </resheader>
  <resheader name="reader">
    <value>System.Resources.ResXResourceReader, System.Windows.Forms, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089</value>
  </resheader>
  <resheader name="writer">
    <value>System.Resources.ResXResourceWriter, System.Windows.Forms, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089</value>
  </resheader>
  <data name="o1-mini" xml:space="preserve">
    <value>The o1 series of models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</value>
  </data>
  <data name="chatgpt-4o-latest" xml:space="preserve">
    <value>Model continuously points to the version of GPT-4o used in ChatGPT. It is updated frequently, when there are significant changes to ChatGPT's GPT-4o model. The knowledge cutoff for GPT-4o models is October, 2023.</value>
  </data>
  <data name="gpt-4o" xml:space="preserve">
    <value>gpt-4o (“o” for “omni”) a versatile, high-intelligence flagship model. It accepts both text and image inputs, and produces text outputs (including Structured Outputs).  Image inputs via the o1, gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.</value>
  </data>
  <data name="gpt-4o-2024-05-13" xml:space="preserve">
    <value>gpt-4o-2024-05-13 (“o” for “omni”) a versatile, high-intelligence flagship model. It accepts both text and image inputs, and produces text outputs (including Structured Outputs).  Image inputs via the o1, gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.</value>
  </data>
  <data name="gpt-4o-2024-08-06" xml:space="preserve">
    <value>gpt-4o-2024-08-06 (“o” for “omni”) a versatile, high-intelligence flagship model. It accepts both text and image inputs, and produces text outputs (including Structured Outputs). Image inputs via the o1, gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.</value>
  </data>
  <data name="gpt-4o-2024-11-20" xml:space="preserve">
    <value>GPT-4o-2024-11-20 (“o” for “omni”) is a versatile, high-intelligence flagship model. It accepts both text and image inputs, and produces text outputs (including Structured Outputs). Image inputs via the o1, gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.</value>
  </data>
  <data name="gpt-4o-mini" xml:space="preserve">
    <value>gpt-4o-mini (“o” for “omni”) is a versatile, high-intelligence flagship model. It accepts both text and image inputs, and produces text outputs (including Structured Outputs). Image inputs via the o1, gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.</value>
  </data>
  <data name="gpt-4o-mini-2024-07-18" xml:space="preserve">
    <value>gpt-4o-mini-2024-07-18 (“o” for “omni”) is a versatile, high-intelligence flagship model. It accepts both text and image inputs, and produces text outputs (including Structured Outputs). Image inputs via the o1, gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.</value>
  </data>
  <data name="o1" xml:space="preserve">
    <value>The o1 series of models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user. </value>
  </data>
  <data name="o1-2024-12-17" xml:space="preserve">
    <value>The o1 series of models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought. o1: reasoning model designed to solve hard problems across domains user.</value>
  </data>
  <data name="o1-mini-2024-09-12" xml:space="preserve">
    <value>o1-mini-2024-09-12; The o1 series of models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</value>
  </data>
  <data name="o1-preview" xml:space="preserve">
    <value>o1-preview;  The o1 series of models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</value>
  </data>
  <data name="o1-preview-2024-09-12" xml:space="preserve">
    <value>o1-preview-2024-09-12;  The o1 series of models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.</value>
  </data>
  <data name="gpt-4o-realtime-preview" xml:space="preserve">
    <value>gpt-4o-realtime-preview;   These models are capable of responding to audio and text inputs in realtime over WebRTC or a WebSocket interface. The knowledge cutoff for GPT-4o Realtime models is October, 2023.</value>
  </data>
  <data name="gpt-4o-realtime-preview-2024-12-17" xml:space="preserve">
    <value>gpt-4o-realtime-preview-2024-12-17;  These models are capable of responding to audio and text inputs in realtime over WebRTC or a WebSocket interface. The knowledge cutoff for GPT-4o Realtime models is October, 2023.</value>
  </data>
  <data name="gpt-4o-realtime-preview-2024-10-01" xml:space="preserve">
    <value>gpt-4o-realtime-preview-2024-10-01;  These models are capable of responding to audio and text inputs in realtime over WebRTC or a WebSocket interface. The knowledge cutoff for GPT-4o Realtime models is October, 2023.</value>
  </data>
  <data name="gpt-4o-mini-realtime-preview" xml:space="preserve">
    <value>gpt-4o-mini-realtime-preview;  These models are capable of responding to audio and text inputs in realtime over WebRTC or a WebSocket interface. The knowledge cutoff for GPT-4o Realtime models is October, 2023.</value>
  </data>
  <data name="gpt-4o-mini-realtime-preview-2024-12-17" xml:space="preserve">
    <value>gpt-4o-mini-realtime-preview-2024-12-17;  These models are capable of responding to audio and text inputs in realtime over WebRTC or a WebSocket interface. The knowledge cutoff for GPT-4o Realtime models is October, 2023.</value>
  </data>
  <data name="gpt-4o-audio-preview" xml:space="preserve">
    <value>gpt-4o-audio-preview; These models accept audio inputs and outputs, and can be used in the Chat Completions REST API.</value>
  </data>
  <data name="gpt-4o-audio-preview-2024-12-17" xml:space="preserve">
    <value>gpt-4o-audio-preview-2024-12-17;  These models accept audio inputs and outputs, and can be used in the Chat Completions REST API.</value>
  </data>
  <data name="gpt-4o-audio-preview-2024-10-01" xml:space="preserve">
    <value>gpt-4o-audio-preview-2024-10-01;  These models accept audio inputs and outputs, and can be used in the Chat Completions REST API</value>
  </data>
  <data name="gpt-4-turbo" xml:space="preserve">
    <value>gpt-4-turbo;  GPT-4 is an older version of a high-intelligence GPT model, usable in Chat Completions. Image inputs via the o1, gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.</value>
  </data>
  <data name="gpt-4-turbo-2024-04-09" xml:space="preserve">
    <value>gpt-4-turbo-2024-04-09.  GPT-4 is an older version of a high-intelligence GPT model, usable in Chat Completions. Image inputs via the o1, gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.</value>
  </data>
  <data name="gpt-4-turbo-preview" xml:space="preserve">
    <value>gpt-4-turbo-preview;  GPT-4 is an older version of a high-intelligence GPT model, usable in Chat Completions. Image inputs via the o1, gpt-4o, gpt-4o-mini, chatgpt-4o-latest, or gpt-4-turbo models (or previously gpt-4-vision-preview) are not eligible for zero retention.</value>
  </data>
  <data name="gpt-4-0125-preview" xml:space="preserve">
    <value>gpt-4-0125-preview;  GPT-4 is an older version of a high-intelligence GPT model, usable in Chat Completions. </value>
  </data>
  <data name="gpt-4-1106-preview" xml:space="preserve">
    <value>gpt-4-1106-preview; GPT-4 is an older version of a high-intelligence GPT model, usable in Chat Completions. </value>
  </data>
  <data name="gpt-4" xml:space="preserve">
    <value>;  GPT-4 is an older version of a high-intelligence GPT model, usable in Chat Completions. </value>
  </data>
  <data name="gpt-4-0613" xml:space="preserve">
    <value>gpt-4 is an older version of a high-intelligence GPT model, usable in Chat Completions. </value>
  </data>
  <data name="gpt-4-0314" xml:space="preserve">
    <value>gpt-4-0314;  GPT-4 is an older version of a high-intelligence GPT model, usable in Chat Completions. </value>
  </data>
  <data name="gpt-3.5-turbo-0125" xml:space="preserve">
    <value>gpt-3.5-turbo-0125; GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well.</value>
  </data>
  <data name="gpt-3.5-turbo" xml:space="preserve">
    <value>gpt-3.5-turbo; Currently points to gpt-3.5-turbo-0125.</value>
  </data>
  <data name="dall-e-3" xml:space="preserve">
    <value>dall-e-3;  DALL·E is a AI system that can create realistic images and art from a description in natural language. DALL·E 3 currently supports the ability, given a prompt, to create a new image with a specific size. DALL·E 2 also support the ability to edit an existing image, or create variations of a user provided image.</value>
  </data>
  <data name="dall-e-2" xml:space="preserve">
    <value>dall-e-2;  DALL·E is a AI system that can create realistic images and art from a description in natural language. DALL·E 3 currently supports the ability, given a prompt, to create a new image with a specific size. DALL·E 2 also support the ability to edit an existing image, or create variations of a user provided image.   </value>
  </data>
  <data name="tts-1" xml:space="preserve">
    <value>tts-1;  TTS is an AI model that converts text to natural sounding spoken text. We offer two different model variates, tts-1 is optimized for real time text to speech use cases and tts-1-hd is optimized for quality. These models can be used with the Speech endpoint in the Audio API. Audio outputs are stored for 1 hour to enable multi-turn conversations, and are not currently eligible for zero retention.</value>
  </data>
  <data name="tts-1-hd" xml:space="preserve">
    <value>tts-1-hd; TTS is an AI model that converts text to natural sounding spoken text. We offer two different model variates, tts-1 is optimized for real time text to speech use cases and tts-1-hd is optimized for quality. These models can be used with the Speech endpoint in the Audio API. Audio outputs are stored for 1 hour to enable multi-turn conversations, and are not currently eligible for zero retention.</value>
  </data>
  <data name="whisper-1" xml:space="preserve">
    <value>Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name. Audio outputs are stored for 1 hour to enable multi-turn conversations, and are not currently eligible for zero retention.</value>
  </data>
  <data name="whisper-2" xml:space="preserve">
    <value>whisper-2;  Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification. The Whisper v2-large model is currently available through our API with the whisper-1 model name. Audio outputs are stored for 1 hour to enable multi-turn conversations, and are not currently eligible for zero retention.</value>
  </data>
  <data name="text-embedding-3-large" xml:space="preserve">
    <value>text-embedding-3-large;  Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks</value>
  </data>
  <data name="text-embedding-3-small" xml:space="preserve">
    <value>text-embedding-3-small;  Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.</value>
  </data>
  <data name="text-embedding-ada-002" xml:space="preserve">
    <value>text-embedding-ada-002; Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.</value>
  </data>
  <data name="babbage-002" xml:space="preserve">
    <value>babbage-002Replacement for the GPT-3 ada and babbage base models. GPT base models can understand and generate natural language or code but are not trained with instruction following. These models are made to be replacements for the original GPT-3 base models and use the legacy Completions API. </value>
  </data>
  <data name="davinci-002" xml:space="preserve">
    <value>davinci-002;  GPT base models can understand and generate natural language or code but are not trained with instruction following. These models are made to be replacements for the original GPT-3 base models and use the legacy Completions API. </value>
  </data>
  <data name="davinci-003" xml:space="preserve">
    <value>davinci-003;  GPT base models can understand and generate natural language or code but are not trained with instruction following. These models are made to be replacements for the original GPT-3 base models and use the legacy Completions API. </value>
  </data>
  <data name="curie-001" xml:space="preserve">
    <value>curie-001;  GPT base models can understand and generate natural language or code but are not trained with instruction following. These models are made to be replacements for the original GPT-3 base models and use the legacy Completions API. </value>
  </data>
</root>